{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-palmer",
   "metadata": {},
   "source": [
    "# Read Proceedings Information\n",
    "Update the `proceedingsInfo.csv` file according to your conference and the filed downloaded from PCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCommittee = pd.read_csv(\"./data/committee.csv\")\n",
    "dfCommittee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfVenues = pd.read_csv(\"./data/proceedingsInfo.csv\")\n",
    "dfVenues = dfVenues.sort_values(\"Order\")\n",
    "dfVenues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-mortality",
   "metadata": {},
   "source": [
    "# Generate Committee Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommittee(PCSId):\n",
    "    path = f\"./data-PCS/{PCSId}_committee.csv\"\n",
    "    if (not os.path.isfile(path)):\n",
    "        print(f\"{PCSId} has no committee file\")\n",
    "        return []\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    df = df[df[\"Reviews assigned\"] != 0]\n",
    "    \n",
    "    if (len(df) == 0):\n",
    "        print(f\"{PCSId} has no reviewers\")\n",
    "        return []\n",
    "\n",
    "    lstText = []\n",
    "    df[\"Family name\"] = df[\"Family name\"].str.title()\n",
    "    df[\"First name\"] = df[\"First name\"].str.title()\n",
    "    #df[\"Middle name\"] = df[\"Middle name\"].str.title()\n",
    "    df = df.sort_values([\"Family name\", \"First name\", \"Middle initial\"])\n",
    "\n",
    "    for i, e in df.iterrows():\n",
    "        for i in range (1,7):\n",
    "            if isinstance(e[f\"Affil {i} Institution\"], str):\n",
    "                break\n",
    "        aff = \"\"\n",
    "        if isinstance(e[f\"Affil {i} Institution\"], str):\n",
    "            aff = f'{e[f\"Affil {i} Institution\"]}, {e[f\"Affil {i} Country\"]}'\n",
    "\n",
    "        aff = aff.replace(\"&\", \"\\\\&\")\n",
    "        if isinstance(e[\"Middle initial\"], str):\n",
    "            lstText.append(f'{e[\"First name\"]} {e[\"Middle initial\"]} {e[\"Family name\"]}, \\\\emph{{{aff}}}\\\\\\\\')\n",
    "        else:\n",
    "            lstText.append(f'{e[\"First name\"]} {e[\"Family name\"]}, \\\\emph{{{aff}}}\\\\\\\\')\n",
    "\n",
    "    if (len(df) > 50):   \n",
    "        lstText.insert(0, \"\\\\begin{multicols}{2}\")\n",
    "        lstText.append(\"\\end{multicols}\")\n",
    "    else:\n",
    "        \n",
    "        lstText.insert(0, \"%\\\\begin{multicols}{2}\")\n",
    "        lstText.append(\"%\\end{multicols}\")\n",
    "    lstText.append(\"\")\n",
    "    return lstText\n",
    "\n",
    "def getReviews(PCSId):\n",
    "    path = f\"./data-PCS/{PCSId}_reviewers.csv\"\n",
    "    if (not os.path.isfile(path)):\n",
    "        print(f\"{PCSId} has no reviwer file\")\n",
    "        return []\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    df = df[df[\"Reviews assigned\"] != 0]\n",
    "    \n",
    "    if (len(df) == 0):\n",
    "        print(f\"{PCSId} has no reviewers\")\n",
    "        return []\n",
    "    \n",
    "    # Ensure that all names start with a capital first latter.\n",
    "    df[\"Family name\"] = df[\"Family name\"].apply(lambda x: x[0].title()+x[1:] if len(x)>2 else x)\n",
    "    df[\"First name\"] = df[\"First name\"].apply(lambda x: x[0].title()+x[1:] if len(x)>2 else x)\n",
    "    if \"Middle name\"in df.columns:\n",
    "        df[\"Middle name\"] = df[\"Middle name\"].apply(lambda x: x[0].title()+x[1:] if len(x)>2 else x)\n",
    "    \n",
    "    df = df.sort_values([\"Family name\", \"First name\", \"Middle initial\"])\n",
    "    lstText = []\n",
    "    lstText.append(\"\\\\begin{multicols}{3}\")\n",
    "    for i, e in df.iterrows():\n",
    "        if isinstance(e[\"Middle initial\"], str):\n",
    "            lstText.append(f'{e[\"First name\"]} {e[\"Middle initial\"]} {e[\"Family name\"]}\\\\\\\\')\n",
    "        else:\n",
    "            lstText.append(f'{e[\"First name\"]} {e[\"Family name\"]}\\\\\\\\')\n",
    "    lstText.append(\"\\\\end{multicols}\")\n",
    "    lstText.append(\"\")\n",
    "    return lstText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "lastPosition = \"\"\n",
    "\n",
    "lstExport = []\n",
    "lstExport.append(\"% Please list all organization committee members and their respected roles below. The best source to fill in this document is the conference webpage.\")\n",
    "\n",
    "for i, e in dfCommittee.iterrows():\n",
    "    \n",
    "    if (lastPosition != e.Position):\n",
    "        lastPosition = e.Position\n",
    "        lstExport.append(\"\")\n",
    "        lstExport.append(f\"\\subsection{{{e.Position}}}\")\n",
    "    \n",
    "    lstExport.append(f'{e.Name}, \\\\emph{{{e.Affiliation}, {e.Country}}}\\\\\\\\')\n",
    "    \n",
    "if (len(lstExport) > 0):\n",
    "        with open(f'committee/committee-organizer.tex', 'w', encoding=\"utf-8\") as fp:\n",
    "            fp.write('\\n'.join(lstExport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in dfVenues.iterrows():\n",
    "    lstExport = []\n",
    "    lstExport.append(\"% If this venue/track has subcommittees, you might want to split the \\subsection{Committee Member} into different \\subsubsections for the different committees.\")\n",
    "    lstExport.append(\"\")    \n",
    "    dfX = dfCommittee[dfCommittee.VenueId == e.VenueId]\n",
    "\n",
    "    if (len(dfX) > 0):\n",
    "        lstExport.append(f\"\\\\subsection{{{e.Name} Chairs}}\")\n",
    "        for j, c in dfX.iterrows():\n",
    "            lstExport.append(f\"{c.Name}, \\emph{{{c.Affiliation}, {c.Country}}}\\\\\\\\\")\n",
    "    else: \n",
    "        print(f\"WARNING: {e.Name} has no chairs assigned to it. use the VenueId '{e.VenueId}' and assign them in the ./data/committee.csv to the respective chair(s)\")\n",
    "    \n",
    "    lstExport.append(\"\")\n",
    "    lstExport.append(\"\")\n",
    "    \n",
    "    commitee = getCommittee(e.PCSId)\n",
    "    if (len(commitee) > 0):\n",
    "        lstExport.append(f\"\\subsection{{{e.NameCommittee}}}\")\n",
    "        lstExport.extend(commitee)\n",
    "\n",
    "    reviewers = getReviews(e.PCSId)\n",
    "    if (len(reviewers) > 0):\n",
    "        lstExport.append(f\"\\subsection{{{e.NameReviewers}}}\")\n",
    "        lstExport.extend(reviewers)\n",
    "    \n",
    "    if (len(lstExport) > 0):\n",
    "        with open(f'committee/committee-{e.VenueId}.tex', 'w', encoding=\"utf-8\") as fp:\n",
    "            fp.write('\\n'.join(lstExport))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-basics",
   "metadata": {},
   "source": [
    "# Loading ACM E-Rights CSV File\n",
    "To get the CSV, navigate to https://cms.acm.org/cms_proceeding_papers_public.cfm?proceedingID=YOURPROCEEDINGSID&confID=YOURCONFERENCEID, at the top left press the `Create CSV` button and copy-and-pasted the content from the new window into the `export.csv` in the `data-erights` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfACM = pd.read_csv(\"./data-erights/export.csv\")\n",
    "\n",
    "## Old code to read the ACM export file;. The format was not a valid CSV.\n",
    "# file1 = open(\"./data-erights/export.csv\", 'r')\n",
    "# lines = file1.readlines()\n",
    "# lstLines = []\n",
    "# for x in lines[1:]:\n",
    "#     e = x[1:].split('\",\"')\n",
    "\n",
    "#     if (\"WITHDRAWN\" in x):\n",
    "#         print(\"WITHDRAWN\", x.split(\",\")[0].replace('\"', \"\"))\n",
    "#     elif (\"DUPLICATE INSERTS\" in x):\n",
    "#         print(\"DUPLICATE INSERTS\", x.split(\",\")[0].replace('\"', \"\"))  \n",
    "#     else:\n",
    "#         lstLines.append(e)       \n",
    "        \n",
    "\n",
    "# columns = lines[0]\n",
    "# columns = columns.replace('\"', \"\").replace(' ', \"\").replace(\"\\n\", \"\").split(\",\")\n",
    "# if \"ACMNo.\" in columns:\n",
    "#     print(\"Warning: It seems you are using a old e-rights export format. Please update the export format.\")\n",
    "\n",
    "# print(columns)\n",
    "# dfACM = pd.DataFrame(lstLines)\n",
    "# dfACM.columns = columns\n",
    "\n",
    "\n",
    "dfACM = dfACM.rename(columns={\"Contact No.\": \"ID\", \"ACM No.\": \"ACMNo\"})\n",
    "\n",
    "dX = dfACM[dfACM[\"Rights Granted\"].str.startswith(\"WITHDRAWN\")]\n",
    "if (len(dX) > 0):\n",
    "    print(f'WITHDRAWN: {dX[\"ID\"].values}')\n",
    "dfACM = dfACM[~dfACM[\"Rights Granted\"].str.startswith(\"WITHDRAWN\")]\n",
    "\n",
    "dX = dfACM[dfACM[\"Rights Granted\"].str.startswith(\"DUPLICATE INSERTS\")]\n",
    "if (len(dX) > 0):\n",
    "    print(f'DUPLICATE INSERTS: {dX[\"ID\"].values}')\n",
    "dfACM = dfACM[~dfACM[\"Rights Granted\"].str.startswith(\"DUPLICATE INSERTS\")]\n",
    "\n",
    "dfACM['Email'] = dfACM.Email.apply(lambda x: x.split(\" \")[0])\n",
    "dfACM['Signed'] = dfACM['Non-ACM Copyright'].apply(lambda x: x[:-3]) != \"load For\"\n",
    "dfACM.Title = dfACM.Title.apply(lambda x: x.split(\" \\\\setcopyright{\")[0])\n",
    "\n",
    "dfACM[\"Prefix\"] = dfACM.ID.apply(lambda x: re.sub(r'[0-9]', '', x))\n",
    "\n",
    "dfACM[\"TitleRaw\"] = dfACM.Title.str.replace('\"', '')\n",
    "dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.replace('[', '', regex=False)\n",
    "\n",
    "## Might needs to be applied to fix LaTex issues.\n",
    "#dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.replace('`', '', regex=False)\n",
    "#dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.replace('“', '', regex=False)\n",
    "#dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.replace(\"'\", '', regex=False)\n",
    "#dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.replace('(', '', regex=False)\n",
    "#dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.replace('#', '', regex=False)\n",
    "#dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.lower()\n",
    "\n",
    "# Remove duplicates, this is possible when the contact authors can not sign the copyright for all authors.\n",
    "dfACM = dfACM.drop_duplicates(\"ID\") \n",
    "\n",
    "dfACM = dfACM[[\"ACMNo\", \"ID\", \"Prefix\", \"Title\", \"Author\", \"DOI\"]]\n",
    "\n",
    "dfACM.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(dfACM, dfVenues[[\"Prefix\", \"Name\", \"VenueId\"]], on=\"Prefix\")\n",
    "\n",
    "myOrder = CategoricalDtype(\n",
    "    dfVenues.sort_values(\"Order\").Prefix.to_list(), \n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "df.Prefix = df.Prefix.astype(myOrder)\n",
    "df = df.sort_values([\"Prefix\", \"Title\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-portuguese",
   "metadata": {},
   "source": [
    "# Load Session Data From QOALA\n",
    "QOALA is the SIGCHI tool to schedule conferences, see https://services.sigchi.org/qoala. Exort the session data from QOALA as `.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exportFileQOALA = \"./data-QOALA/export.json\"\n",
    "\n",
    "if (os.path.isfile(exportFileQOALA)):\n",
    "    print(\"Info: Using QOALA data for the sessions\")\n",
    "    with open(exportFileQOALA, 'r', encoding=\"utf-8\") as f:\n",
    "        qoala = json.load(f)\n",
    "        \n",
    "    if (qoala[\"schemeVersion\"] != 7):\n",
    "        print(\"WARNING: This script was not tested with the QOALA expert scheme version. It might not fully working.\")\n",
    "        \n",
    "    dfQPapers = pd.DataFrame(qoala[\"contents\"])\n",
    "    \n",
    "    if len(dfQPapers) > 0:\n",
    "    \n",
    "        dfX = dfQPapers[dfQPapers.sessionIds.apply(lambda x: len(x) > 1)]\n",
    "        if len (dfX) > 0:\n",
    "            print(f'WARNING: The following papers are in more than one session: {dfX.importedId.to_list()}')\n",
    "    \n",
    "        dfQPapers.sessionIds = dfQPapers.sessionIds.apply(lambda x: x[0])\n",
    "    \n",
    "        dfQSessions = pd.DataFrame(qoala[\"sessions\"])\n",
    "        dfQSessions = dfQSessions.rename(columns={\"id\":\"sessionId\", \"name\":\"SessionName\"})\n",
    "    \n",
    "        dfQoala = pd.merge(dfQPapers, dfQSessions[[\"sessionId\", \"SessionName\"]], left_on=\"sessionIds\", right_on=\"sessionId\", how=\"left\")[[\"importedId\", \"title\", \"SessionName\"]]\n",
    "        dfQoala[\"PCSId\"] = dfQoala.importedId.apply(lambda x: x.split(\"-\")[0])\n",
    "        dfQoala[\"IdRaw\"] = dfQoala.importedId.apply(lambda x: x.split(\"-\")[1])\n",
    "    \n",
    "        myMap = dfVenues.set_index(\"PCSId\")[\"Prefix\"].to_dict()\n",
    "        dfQoala[\"Prefix\"] = dfQoala.PCSId.map(myMap)\n",
    "    \n",
    "        dfQoala[\"ID\"] = dfQoala[\"Prefix\"] + dfQoala[\"IdRaw\"]\n",
    "        dfQoala.head()\n",
    "    else:\n",
    "        print(\"WARNING: QOALA data for the papers is not available.\")\n",
    "        \n",
    "else:\n",
    "    for i, e in dfVenues.iterrows():\n",
    "        df.loc[df.Prefix == e.Prefix, \"SessionName\"] = e.Name  \n",
    "    \n",
    "    print(\"WARNING: QOALA data for the papers is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'dfQoala' in globals():\n",
    "    if (\"SessionName\" in df.columns):\n",
    "        del df[\"SessionName\"]\n",
    "    df = pd.merge(df, dfQoala[[\"ID\", \"SessionName\"]], on=\"ID\", how=\"outer\")\n",
    "    \n",
    "    for i, e in dfVenues.iterrows():\n",
    "        if (not e.UseQOALASessions):\n",
    "            df.loc[df.Prefix == e.Prefix, \"SessionName\"] = e.Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb294b1d-2483-4376-ba1d-140c0a8e50ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Prefix.value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-passion",
   "metadata": {},
   "source": [
    "# Generate the TOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstExport = []\n",
    "lstExport.append(\"%% This file lists all items that are in the proceedings in the ACM DL. This again varies depending on if you have a companion proceedings or not.\")\n",
    "lstExport.append(\"%% Note: For conferences that will publish the full papers in PACMHCI, then the full papers are not to be listed here.\")\n",
    "lstExport.append(\"\")\n",
    "lastPrefix = \"\"\n",
    "counterTOC = 1\n",
    "\n",
    "tapsTOC=[]\n",
    "lstAuthorIndex = []\n",
    "lstDetails = []\n",
    "for j, f in dfVenues.sort_values(\"Order\").iterrows():\n",
    "    dfTrack = df[df.Prefix == f.Prefix]\n",
    "    \n",
    "    counter = 1\n",
    "    lastSessionName = \"\"\n",
    "    \n",
    "    lstExport.append(f'\\\\subsection{{{f.Name}}}')\n",
    "    \n",
    "    if (f.UseQOALASessions):\n",
    "        dfTrack = dfTrack.sort_values([\"SessionName\", \"Title\"])\n",
    "    else: \n",
    "        dfTrack = dfTrack.sort_values([\"Title\"])\n",
    "    \n",
    "    for i, e in dfTrack.iterrows():\n",
    "        \n",
    "        if (lastSessionName != e.SessionName) & (f.UseQOALASessions):\n",
    "            lastSessionName = e.SessionName\n",
    "            if (counter != 1):\n",
    "                lstExport.pop()\n",
    "                lstExport.append(f'\\\\end{{enumerate}}')\n",
    "                lstExport.append(\"\")\n",
    "            lstExport.append(f'\\\\subsubsection{{{e.SessionName}}}')\n",
    "            lstExport.append(f'\\\\begin{{enumerate}}')\n",
    "        elif (counter == 1):\n",
    "            lstExport.append(f'\\\\begin{{enumerate}}')\n",
    "        \n",
    "        lstExport.append(f'%PCS ID: {e.ID}')\n",
    "        TOC_ID = f\"{e.VenueId.upper()}{counter:03}\"\n",
    "        lstExport.append(f'\\\\item[\\\\href{{{e.DOI}}}{{\\\\textbf{{{TOC_ID}}}}}]')\n",
    "\n",
    "        xx = f'\\\\href{{{e.DOI}}}{{\\\\textbf{{{e.Title}}}}}\\\\\\\\'\n",
    "        lstExport.append(xx.replace(\"&\", \"\\\\&\").replace(\"#\", \"\\\\#\"))\n",
    "        for x in e.Author.split(\";\"):\n",
    "            x = x.split(\":\")\n",
    "            \n",
    "            if (len(x) > 1):\n",
    "                xx = f\"{x[0]}, \\\\emph{{{x[1]}}}\\\\\\\\\"\n",
    "            else:\n",
    "                xx = f\"{x[0]}\\\\\\\\\"\n",
    "                \n",
    "            lstAuthorIndex.append([x[0], TOC_ID])\n",
    "            lstExport.append(xx.replace(\"&\", \"\\\\&\").replace(\"#\", \"\\\\#\").replace(\",\", \", \").replace(\"  \", \" \"))\n",
    "                \n",
    "\n",
    "        xx = f'\\\\textbf{{DOI:}} \\\\href{{{e.DOI}}}{{{e.DOI}}}\\\\\\\\'\n",
    "        lstExport.append(xx.replace(\"&\", \"\\\\&\").replace(\"#\", \"\\\\#\"))\n",
    "        lstExport.append(\"\")\n",
    "\n",
    "        counter = counter + 1\n",
    "        counterTOC = counterTOC + 1\n",
    "\n",
    "        lstDetails.append({\"ID\": e.ID, \"TOC_ID\":TOC_ID, \"Order\":counterTOC})\n",
    "    \n",
    "    lstExport.pop()\n",
    "    lstExport.append(f'\\\\end{{enumerate}}')\n",
    "    lstExport.append(\"\")\n",
    "    lstExport.append(\"\")\n",
    "\n",
    "if (len(lstExport) > 0):\n",
    "    with open(f'./content/content.tex', 'w', encoding=\"utf-8\") as fp:\n",
    "        fp.write('\\n'.join(lstExport))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-cradle",
   "metadata": {},
   "source": [
    "# Generate the TOC for APTARA\n",
    "This puts the TOC entries into \"sessions.\" Sessions are the structure ACM used in the ACM DL; for example, see the structure here on the left side: https://dl.acm.org/doi/proceedings/10.1145/3544548\n",
    "The list is to be sent via email to APTARA so they can sort them accordingly and prepare for the ACM upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAptaraExport = pd.merge(df, pd.DataFrame(lstDetails), on=\"ID\")\n",
    "dfAptaraExport = dfAptaraExport[[\"Order\", \"ACMNo\", \"ID\", \"TOC_ID\", \"Title\", \"SessionName\", \"DOI\"]]\n",
    "dfAptaraExport = dfAptaraExport.sort_values([\"Order\", \"TOC_ID\"])\n",
    "dfAptaraExport.to_csv(\"./export/TOC-for-APTARA.csv\", index=False)\n",
    "dfAptaraExport"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
